# GPT2-FineTuning

This repository showcases a fine-tuned GPT-2 language model designed for generating code snippets based on specific programming prompts. The training process incorporates a novel keyword-weighted loss function, emphasizing crucial tokens related to programming libraries or functions. Leveraging the Hugging Face Transformers library and Accelerated Inference API, this project provides an efficient and powerful tool for generating coherent code tailored to user-defined tasks. Explore the codebase, fine-tune the model on your dataset, and experience the capabilities of this keyword-aware GPT-2 model for code generation.
